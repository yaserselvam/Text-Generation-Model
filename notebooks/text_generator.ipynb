{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2713af8f-f875-409a-9e8d-44db2d6056df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "dataset, info = tfds.load('tiny_shakespeare', with_info=True, as_supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ca9b02-9c1e-440a-8858-d1e0d97cbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = next(iter(dataset['train']))['text'].numpy().decode('utf-8')\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddb949e-22aa-4cad-a17d-d6996d544176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11469d2-89ab-401d-b7a0-a96c0e9151a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e67a9ec-e5bf-4159-b075-8def4fcb83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    inputs = tf.keras.Input(batch_shape=(batch_size, None))\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ef0a1c-ecd1-43c1-93fd-0c20c14f26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d5caa6f-eb72-468e-9a78-6fc9d6052b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 1s/step - loss: 2.8897\n",
      "Epoch 2/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 1s/step - loss: 1.8758\n",
      "Epoch 3/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 1s/step - loss: 1.6164\n",
      "Epoch 4/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 1s/step - loss: 1.4861\n",
      "Epoch 5/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 1s/step - loss: 1.4078\n",
      "Epoch 6/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 1s/step - loss: 1.3530\n",
      "Epoch 7/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 1s/step - loss: 1.3073\n",
      "Epoch 8/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 1s/step - loss: 1.2720\n",
      "Epoch 9/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2977s\u001b[0m 19s/step - loss: 1.2394\n",
      "Epoch 10/10\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 1s/step - loss: 1.2019\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True  # Only save weights\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f099b47-5511-401c-8603-6e17eacbd181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from ./training_checkpoints/ckpt_10.weights.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Manually find the latest checkpoint file\n",
    "checkpoint_files = os.listdir(checkpoint_dir)\n",
    "checkpoint_files = [f for f in checkpoint_files if f.endswith(\".weights.h5\")]\n",
    "checkpoint_files.sort(key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "latest_checkpoint = checkpoint_files[-1] if checkpoint_files else None\n",
    "\n",
    "# Build the model\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Load weights if a checkpoint is found\n",
    "if latest_checkpoint:\n",
    "    latest_checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    model.load_weights(latest_checkpoint_path)\n",
    "    print(f\"Loaded weights from {latest_checkpoint_path}\")\n",
    "else:\n",
    "    print(\"No valid checkpoint found.\")\n",
    "\n",
    "# Build the model for inference\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e95a30d9-5686-4adb-ac3e-c791de8406e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUEEN: So, lets end this to ever more of you;\n",
      "And she is so I, no. How oway's good deess!\n",
      "Emer, when thou broke! Darkncif Milerey's liquoth.\n",
      "Mark him, we his eyes was too much aprain?\n",
      "In play apparelly impress'd by me; but thou shalt be\n",
      "In pain and woman tyrant madam in peace.\n",
      "\n",
      "Second Murderer:\n",
      "O my Lord of Wariame.\n",
      "\n",
      "Stird Augard:\n",
      "In the Warwick where thou werp death it seems our king,\n",
      "And even so noble Romeo!' thy sun\n",
      "Ere now grief to their event, look to know\n",
      "you rohe many flant and than that places\n",
      "To meer myself or each other thee decline,\n",
      "With that can hear you born him hither; before thy pry God in heaven,\n",
      "I heard you see good Blancaster; they have too Norfolk'd business\n",
      "From such apputy to his uncle woeld so till\n",
      "Than climate youe dare only their conversation,\n",
      "Resterbert my names and high deserts, past Lord Angelo,\n",
      "Who had so made it first? Where he and unknessed change\n",
      "Was weap's revoil upon thee. But camal addly be gone.\n",
      "\n",
      "HENRY BORINGBROKE:\n",
      "Good, worrior is a respect pay the night:\n",
      "Go to that in itse\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 1000\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=u\"QUEEN: So, lets end this\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
